# PDF Document Layout Detection Configuration

# Model settings
model:
  # DocLayout-YOLO model path (will auto-download from HuggingFace)
  doclayout_model: "juliozhao/DocLayout-YOLO-DocStructBench"

  # Confidence threshold for detections
  confidence_threshold: 0.25

  # IOU threshold for NMS
  iou_threshold: 0.45

# Device settings
device:
  # Use MPS for Mac M-series chips, "cpu" for CPU only
  type: "cuda"

# PDF conversion settings
pdf:
  # DPI for PDF to image conversion
  dpi: 200

  # Output image format
  format: "png"

# Path settings
paths:
  # Input PDF directory
  input_dir: "data/papers"

  # Output root directory (each PDF gets its own subdirectory)
  output_dir: "data/output"

  # Subdirectory names within each PDF's output folder
  pages_subdir: "pages"           # Original page images
  annotated_subdir: "annotated"   # Visualization images
  extractions_subdir: "extractions"  # Extracted figures/tables

  # Models directory
  models_dir: "models"

  # Benchmark data directory
  benchmark_dir: "data/benchmark"

# Extraction settings
extraction:
  # Padding in pixels to add around cropped images
  image_padding: 5

  # Maximum vertical distance in pixels for caption matching
  # (100px at 200 DPI is approximately 0.5 inch)
  max_caption_distance: 100.0

  # Minimum horizontal overlap ratio for caption matching (0-1)
  min_horizontal_overlap: 0.3

  # Caption search direction settings
  # Options: "below", "above", "both"
  caption_search:
    # Figures typically have captions below
    figure_direction: "below"
    # Tables typically have captions above (academic papers convention)
    table_direction: "above"

# Visualization settings
visualization:
  # Enable visualization output
  enabled: true

  # Bounding box line thickness
  line_thickness: 2

  # Font scale for labels
  font_scale: 0.6

  # Show confidence scores
  show_confidence: true

# DocLayout-YOLO DocStructBench classes (10 classes)
# Model automatically detects class names from the model file
doclayout_classes:
  0: "Title"
  1: "Plain-Text"
  2: "Abandon"
  3: "Figure"
  4: "Figure-Caption"
  5: "Table"
  6: "Table-Caption"
  7: "Table-Footnote"
  8: "Isolate-Formula"
  9: "Formula-Caption"

# VLM Annotation settings
vlm_annotation:
  # Default VLM backend (ollama, openai, anthropic)
  default_backend: "ollama"

  # OpenAI settings
  openai:
    model: "gpt-4o"
    max_tokens: 1000
    temperature: 0.1

  # Anthropic settings
  anthropic:
    model: "claude-sonnet-4-20250514"
    max_tokens: 1000
    temperature: 0.1

  # Ollama settings (local, free)
  ollama:
    model: "llava:13b"
    timeout: 120

  # Annotation confidence threshold
  confidence_threshold: 0.7

# Caption Matching Benchmark settings
caption_benchmark:
  # Default benchmark directory
  benchmark_dir: "benchmark/caption-matching"

  # Default confidence threshold for ground truth matches
  confidence_threshold: 0.7

  # Default output format for reports
  output_format: "both"  # json, markdown, or both

  # VLM settings for batch annotation
  vlm:
    default_backend: "openai"
    skip_existing: true

# Class colors (BGR format for OpenCV)
class_colors:
  Title: [128, 0, 128]        # Purple
  Plain-Text: [0, 180, 0]     # Dark Green
  Abandon: [64, 64, 64]       # Dark Gray
  Figure: [0, 255, 0]         # Green
  Figure-Caption: [255, 200, 0] # Light Orange
  Table: [0, 0, 255]          # Blue
  Table-Caption: [0, 100, 255] # Light Blue
  Table-Footnote: [128, 128, 128] # Gray
  Isolate-Formula: [255, 0, 255]  # Magenta
  Formula-Caption: [200, 0, 200]  # Dark Magenta
